\# üöÄ LiteLLM-Ollama Management Tools



> \*\*Effortlessly manage your Ollama models in LiteLLM proxy with these powerful Python utilities\*\*



\[!\[Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://www.python.org/)

\[!\[License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

\[!\[LiteLLM](https://img.shields.io/badge/LiteLLM-Compatible-orange.svg)](https://litellm.ai/)

\[!\[Ollama](https://img.shields.io/badge/Ollama-Compatible-red.svg)](https://ollama.ai/)



\## ‚ú® Features



\- üîÑ \*\*Bulk Model Management\*\*: Add all your Ollama models to LiteLLM with a single command

\- üßπ \*\*Duplicate Cleanup\*\*: Automatically detect and remove duplicate models

\- üéØ \*\*Smart Detection\*\*: Only add new models, skip existing ones

\- üîç \*\*Dry Run Mode\*\*: Preview changes before executing them

\- üí™ \*\*Force Mode\*\*: Override duplicate detection when needed

\- üé® \*\*Beautiful CLI\*\*: Rich output with emojis and progress indicators

\- üõ°Ô∏è \*\*Error Handling\*\*: Robust error handling and detailed feedback



\## üìã Table of Contents



\- \[üöÄ LiteLLM-Ollama Management Tools](#-litellm-ollama-management-tools)

&nbsp; - \[‚ú® Features](#-features)

&nbsp; - \[üìã Table of Contents](#-table-of-contents)

&nbsp; - \[üèóÔ∏è Prerequisites](#Ô∏è-prerequisites)

&nbsp; - \[‚ö° Quick Start](#-quick-start)

&nbsp; - \[üìö Scripts Overview](#-scripts-overview)

&nbsp;   - \[üßπ cleanup\\\_duplicates.py](#-cleanup\_duplicatespy)

&nbsp;   - \[‚ûï add\\\_new\\\_models.py](#-add\_new\_modelspy)

&nbsp; - \[üîß Usage Examples](#-usage-examples)

&nbsp;   - \[Adding Models from Ollama](#adding-models-from-ollama)

&nbsp;   - \[Cleaning Up Duplicates](#cleaning-up-duplicates)

&nbsp;   - \[Real-world Workflow](#real-world-workflow)

&nbsp; - \[‚öôÔ∏è Command Line Options](#Ô∏è-command-line-options)

&nbsp;   - \[Common Options](#common-options)

&nbsp;   - \[add\\\_new\\\_models.py Specific](#add\_new\_modelspy-specific)

&nbsp;   - \[cleanup\\\_duplicates.py Specific](#cleanup\_duplicatespy-specific)

&nbsp; - \[üéØ Use Cases](#-use-cases)

&nbsp; - \[üîç Troubleshooting](#-troubleshooting)

&nbsp; - \[üìñ API Reference](#-api-reference)

&nbsp; - \[ü§ù Contributing](#-contributing)

&nbsp; - \[üìÑ License](#-license)



\## üèóÔ∏è Prerequisites



\- \*\*Python 3.7+\*\* installed on your system

\- \*\*Ollama server\*\* running and accessible

\- \*\*LiteLLM proxy\*\* running with API access

\- \*\*LiteLLM API key/Master key\*\* for authentication



\### Required Python Packages



```bash

pip install requests

```



\## ‚ö° Quick Start



1\. \*\*Clone or download\*\* the scripts to your local machine



2\. \*\*Get your LiteLLM API key\*\* from your LiteLLM proxy setup



3\. \*\*Add all your Ollama models\*\* to LiteLLM:

&nbsp;  ```bash

&nbsp;  python3 add\_new\_models.py \\

&nbsp;    --ollama-url http://192.168.1.250:11434 \\

&nbsp;    --litellm-url http://localhost:4000 \\

&nbsp;    --api-key your-litellm-api-key

&nbsp;  ```



4\. \*\*Clean up any duplicates\*\* (if needed):

&nbsp;  ```bash

&nbsp;  python3 cleanup\_duplicates.py \\

&nbsp;    --ollama-url http://192.168.1.250:11434 \\

&nbsp;    --litellm-url http://localhost:4000 \\

&nbsp;    --api-key your-litellm-api-key

&nbsp;  ```



\## üìö Scripts Overview



\### üßπ cleanup\_duplicates.py



\*\*Purpose\*\*: Remove duplicate models from your LiteLLM proxy



\*\*What it does\*\*:

\- Scans all models in LiteLLM proxy

\- Identifies models with identical names

\- Keeps the first occurrence, removes duplicates

\- Provides detailed reporting of actions taken



\*\*Perfect for\*\*: Cleaning up after manual additions or bulk imports that created duplicates



\### ‚ûï add\_new\_models.py



\*\*Purpose\*\*: Intelligently add Ollama models to LiteLLM proxy



\*\*What it does\*\*:

\- Fetches all available models from your Ollama server

\- Checks existing models in LiteLLM proxy

\- Adds only new models (unless forced)

\- Provides comprehensive progress reporting



\*\*Perfect for\*\*: Initial setup or adding new models after downloading them to Ollama



\## üîß Usage Examples



\### Adding Models from Ollama



```bash

\# Preview what would be added (safe to run)

python3 add\_new\_models.py \\

&nbsp; --ollama-url http://192.168.1.250:11434 \\

&nbsp; --litellm-url http://localhost:4000 \\

&nbsp; --api-key sk-1234567890abcdef \\

&nbsp; --dry-run



\# Add only new models (recommended)

python3 add\_new\_models.py \\

&nbsp; --ollama-url http://192.168.1.250:11434 \\

&nbsp; --litellm-url http://localhost:4000 \\

&nbsp; --api-key sk-1234567890abcdef



\# Force add all models (creates duplicates)

python3 add\_new\_models.py \\

&nbsp; --ollama-url http://192.168.1.250:11434 \\

&nbsp; --litellm-url http://localhost:4000 \\

&nbsp; --api-key sk-1234567890abcdef \\

&nbsp; --force

```



\### Cleaning Up Duplicates



```bash

\# Preview what would be cleaned (safe to run)

python3 cleanup\_duplicates.py \\

&nbsp; --ollama-url http://192.168.1.250:11434 \\

&nbsp; --litellm-url http://localhost:4000 \\

&nbsp; --api-key sk-1234567890abcdef \\

&nbsp; --dry-run



\# Actually remove duplicates

python3 cleanup\_duplicates.py \\

&nbsp; --ollama-url http://192.168.1.250:11434 \\

&nbsp; --litellm-url http://localhost:4000 \\

&nbsp; --api-key sk-1234567890abcdef

```



\### Real-world Workflow



```bash

\# 1. First, see what's currently in Ollama

curl http://192.168.1.250:11434/api/tags | jq '.models\[].name'



\# 2. Preview what would be added to LiteLLM

python3 add\_new\_models.py --ollama-url http://192.168.1.250:11434 --litellm-url http://localhost:4000 --api-key YOUR\_KEY --dry-run



\# 3. Add the new models

python3 add\_new\_models.py --ollama-url http://192.168.1.250:11434 --litellm-url http://localhost:4000 --api-key YOUR\_KEY



\# 4. If you accidentally created duplicates, clean them up

python3 cleanup\_duplicates.py --ollama-url http://192.168.1.250:11434 --litellm-url http://localhost:4000 --api-key YOUR\_KEY --dry-run

python3 cleanup\_duplicates.py --ollama-url http://192.168.1.250:11434 --litellm-url http://localhost:4000 --api-key YOUR\_KEY

```



\## ‚öôÔ∏è Command Line Options



\### Common Options



| Option | Required | Description | Example |

|--------|----------|-------------|---------|

| `--ollama-url` | ‚úÖ | URL of your Ollama server | `http://192.168.1.250:11434` |

| `--litellm-url` | ‚úÖ | URL of your LiteLLM proxy | `http://localhost:4000` |

| `--api-key` | ‚úÖ | LiteLLM API key or master key | `sk-1234567890abcdef` |

| `--dry-run` | ‚ùå | Preview changes without executing | `--dry-run` |



\### add\_new\_models.py Specific



| Option | Description | Use Case |

|--------|-------------|----------|

| `--force` | Add all models even if they exist | When you want to intentionally create duplicates |



\### cleanup\_duplicates.py Specific



> No additional options - focused on one task: cleaning duplicates safely



\## üéØ Use Cases



| Scenario | Recommended Approach |

|----------|---------------------|

| \*\*First-time setup\*\* | Use `add\_new\_models.py` to bulk-add all your Ollama models |

| \*\*Regular maintenance\*\* | Use `add\_new\_models.py` after downloading new models to Ollama |

| \*\*After manual additions\*\* | Use `cleanup\_duplicates.py` to remove any duplicates |

| \*\*Migration/Backup restore\*\* | Use `--force` with `add\_new\_models.py`, then `cleanup\_duplicates.py` |

| \*\*Testing changes\*\* | Always use `--dry-run` first to preview changes |



\## üîç Troubleshooting



\### Common Issues



\*\*‚ùå "Failed to get models: 401"\*\*

```bash

\# Check your API key

curl -H "Authorization: Bearer YOUR\_KEY" http://localhost:4000/model/info

```



\*\*‚ùå "Error connecting to Ollama"\*\*

```bash

\# Test Ollama connection

curl http://192.168.1.250:11434/api/tags

```



\*\*‚ùå "No models found in Ollama"\*\*

```bash

\# Check if Ollama has models

ollama list

```



\### Debug Mode



Add verbose output to see exactly what's happening:

```bash

\# Check the raw response structure

python3 -c "

import requests

response = requests.get('http://localhost:4000/model/info', headers={'Authorization': 'Bearer YOUR\_KEY'})

print(response.status\_code)

print(response.text\[:500])

"

```



\### Network Issues



\- Ensure all services are running and accessible

\- Check firewalls and network connectivity

\- Verify URLs don't have typos (common mistake!)



\## üìñ API Reference



\### LiteLLM Endpoints Used



\- `GET /model/info` - Retrieve existing models

\- `POST /model/new` - Add new model

\- `DELETE /model/delete` - Remove model by ID



\### Ollama Endpoints Used



\- `GET /api/tags` - List available models



\### Model Configuration Format



```json

{

&nbsp; "model\_name": "llama2:latest",

&nbsp; "litellm\_params": {

&nbsp;   "model": "ollama/llama2:latest",

&nbsp;   "api\_base": "http://192.168.1.250:11434"

&nbsp; }

}

```



\## ü§ù Contributing



We welcome contributions! Here's how you can help:



1\. \*\*üêõ Report bugs\*\* - Open an issue with details

2\. \*\*üí° Suggest features\*\* - Share your ideas

3\. \*\*üîß Submit PRs\*\* - Fix bugs or add features

4\. \*\*üìö Improve docs\*\* - Help make this README even better



\### Development Setup



```bash

\# Clone the repository

git clone <your-repo-url>



\# Create a virtual environment

python3 -m venv venv

source venv/bin/activate  # On Windows: venv\\Scripts\\activate



\# Install dependencies

pip install requests



\# Run tests

python3 add\_new\_models.py --help

python3 cleanup\_duplicates.py --help

```



\### Code Style



\- Follow PEP 8

\- Use meaningful variable names

\- Add docstrings to functions

\- Include error handling



\## üìÑ License



This project is licensed under the MIT License - see the \[LICENSE](LICENSE) file for details.



---



<div align="center">



\*\*Made with ‚ù§Ô∏è for the LiteLLM and Ollama community\*\*



‚≠ê \*\*Star this repo\*\* if it helped you! ‚≠ê



\[Report Bug](../../issues) ‚Ä¢ \[Request Feature](../../issues) ‚Ä¢ \[Documentation](../../wiki)



</div>

